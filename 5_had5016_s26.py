# -*- coding: utf-8 -*-
"""5-HAD5016-S26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JdOYlr7tU0uKLgRBjcV2W--7-x0E1Xt-

Step 1: Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

"""Step 2: Load Data"""

from google.colab import drive

#mount to google drive
drive.mount("/content/drive")

#define file path
file_path = "/content/drive/MyDrive/Applied ML for PH Data/Week 3/Datathon #2 - Cardiovascular Health.csv" ##replace with your file path

#load csv data in pandas frame
df = pd.read_csv(file_path)

#display first few heads to see if data loaded correctly
df.head()

"""Step 3: Data Exploration"""

description = df.describe()
description

#mean age for the dataset is around 50 years old
#youngest person is 18 and oldest is 82 so no crazy outliers to remove but maybe we could think about excluding those of retirement age (65 and over)?

"""Step 4: Check for missing values"""

# Checking for missing values, in case there are any NA values, in this case there are none
print(df.isnull().sum())

#no missing values to deal with in variables of interest (gender, age, and employment)

"""Step 5: Clean data (rows):
- exclude everyone over 65 (age of retirement)
- exclude "Other" gender (n=1)
"""

df = df[df['age'] <= 65] #exclude everyone over 65
df = df[df['gender'] != 'Other'] #exclude "other" in gender column

df

"""Step 6: Transform gender, ever_married, and residence_type column from categorical to binary"""

from sklearn.preprocessing import StandardScaler, LabelEncoder
le = LabelEncoder()

from sklearn.preprocessing import StandardScaler, LabelEncoder
le = LabelEncoder()

df['gender'] = le.fit_transform(df['gender']) #female = 0

df['ever_married'] = le.fit_transform(df['ever_married']) #No = 0

df['residence_type'] = le.fit_transform(df['residence_type']) #rural = 0

df.head()

df

"""Step 7: One-hot encode employment and smoking status"""

#one hot encoding
df_encoded = pd.get_dummies(df, columns= ['employment', "smoking_status"])
df_encoded.head()

"""Step 8: Clean data (columns) for Model 1
- Remove clinical factors (hypertension, heart disease, random glucose and bmi) to only focus on all lifestyle factors in the dataset that affect stroke (age, gender, employment, smoking status, residence type, and marriage status). Also removed ID as it does not provide any meaningful contribution to the model.
"""

df_clean1 = df_encoded.loc[:, ['age', 'gender', 'employment_Private Sector', "employment_Public Sector", "employment_Self-employed", "employment_Unemployed", 'stroke', "smoking_status_Active Smoker", "smoking_status_Former Smoker", "smoking_status_Never Smoker", "smoking_status_Unknown", "residence_type", "ever_married"]]
df_clean1.head()

"""Step 9: Clean data (columns) for Model 2
- Remove clinical factors (hypertension, heart disease, random glucose and bmi) and select lifestyle factors (smoking status, residence type and marriage status) that may affect stroke to focus on the relationship between stroke and age, gender, and employment. Also removed ID as it does not provide any meaningful contribution to the model.
"""

df_clean2 = df_encoded.loc[:, ['age', 'gender', 'employment_Private Sector', "employment_Public Sector", "employment_Self-employed", "employment_Unemployed", 'stroke']]
df_clean2.head()

"""Step 10: Splitting Data into Training and Testing (Model 1)"""

import sklearn.model_selection as model_selection
from sklearn.model_selection import train_test_split

#Split data into features and target variable
X = df_clean1.drop('stroke', axis=1)
y = df_clean1['stroke']

#Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=42, stratify=y)

#Normalize data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train) #fit and transform on training set
X_test = scaler.transform(X_test) #transform test

"""Step 11: Build Logistic Regression Model
- using L2 (ridge) to prevent overfitting
- L2 handles multicollinearity among correlated categorical predictors
"""

# Training logistic regression model
log_reg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')
log_reg.fit(X_train, y_train)

"""Step 12: Evaluating performance of logistic regression Model 1"""

# Use the model to make predictions on the testing data
y_pred = log_reg.predict(X_test)

# Evaluate the model
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

#640 true negatives, 0 false positives, 18 false negatives (missed strokes) and 0 true positives (correctly detected strokes)
#model accuracy for predicting no stroke is 97% BUT severe class imbalance because precision is 0.00 for stroke

# Visualizing confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='d')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

"""Step 13: Balance Model 1"""

balanced_model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', class_weight='balanced', max_iter=1000)
balanced_model.fit(X_train, y_train)

"""Step 14: Evaluate balanced Model 1"""

# Use the model to make predictions on the testing data
y_pred = balanced_model.predict(X_test)

# Evaluate the model
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

#400 true negatives, 240 false positives, 5 false negatives (missed strokes), 13 true positives (correctly detected strokes)
#63% accuracy

# Visualizing confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='d')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

"""Repeat Steps 10-14 for Model 2

Logistic Regression Model Summary
"""

import statsmodels.api as sm

# Separate features and target variable
X = df_clean1.drop('stroke', axis=1)
y = df_clean1['stroke']

# Convert all columns in X to integer type
X = X.astype(int)

# Drop one category from each one-hot encoded group to avoid multicollinearity
X = X.drop(columns=['employment_Unemployed', 'smoking_status_Unknown'])

# Add a constant term for the intercept
X = sm.add_constant(X)

# Create and fit logistic regression model
log_reg = sm.Logit(y, X).fit()

# Get summary report
summary = log_reg.summary()
print(summary)